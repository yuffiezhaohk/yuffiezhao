{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQo/rKF572GaLu7p5XA3X1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGWbAz-3JdJx"
      },
      "outputs": [],
      "source": [
        "# %% [Cell 1] Imports, Type Definitions, and Global Configuration\n",
        "import math\n",
        "import time\n",
        "import itertools\n",
        "import os\n",
        "import unittest\n",
        "import random\n",
        "from typing import List, Tuple, Dict, Any, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Added for visualization\n",
        "\n",
        "# --- Random seed management function ---\n",
        "def set_global_seeds(seed=42):\n",
        "    \"\"\"Initialize random seeds for TensorFlow, NumPy, and Python random module.\"\"\"\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_global_seeds(42)  # Set reproducible baseline for all subsequent operations\n",
        "\n",
        "# --- Type Aliases for Clarity ---\n",
        "ChoiceSet = Tuple[int, ...]  # Represents a tuple of item indices\n",
        "ModelConfig = Dict[str, Any]\n",
        "\n",
        "# --- Configuration ---\n",
        "# Toggle this: True for quick unit testing (DEBUG), False for full paper reproduction.\n",
        "DEBUG_MODE: bool = False\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    print(\"âš ï¸ RUNNING IN DEBUG MODE: Small scale for verification.\")\n",
        "    CONFIG = {\n",
        "        \"universe_size\": 5,         # J\n",
        "        \"choice_set_size\": 3,       # K\n",
        "        \"obs_per_set\": 10,\n",
        "        \"epochs\": 5,\n",
        "        \"learning_rate\": 1e-3,\n",
        "        \"batch_size\": 32,\n",
        "        \"param_budgets\": [500, 1000],\n",
        "        \"network_depths\": [2, 3]    # L\n",
        "    }\n",
        "    DATA_FILENAME = \"debug_data_final.npz\"\n",
        "else:\n",
        "    print(\"ðŸš¨ RUNNING IN FULL MODE: Full scale paper reproduction.\")\n",
        "    CONFIG = {\n",
        "        \"universe_size\": 20,\n",
        "        \"choice_set_size\": 15,\n",
        "        \"obs_per_set\": 80,\n",
        "        \"epochs\": 500,\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"batch_size\": 1024,\n",
        "        \"param_budgets\": [200000, 500000],\n",
        "        \"network_depths\": [3, 4, 5, 6, 7]\n",
        "    }\n",
        "    DATA_FILENAME = \"full_scale_data_final.npz\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [Cell 2] Helper Function: Architecture Constraint Solver\n",
        "\n",
        "def calculate_layer_width(\n",
        "    network_depth: int,\n",
        "    target_param_count: int,\n",
        "    universe_size: int\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Calculates the optimal number of neurons per hidden layer to satisfy a total parameter constraint.\n",
        "\n",
        "    This function solves the quadratic equation derived from the ResNet architecture parameter count:\n",
        "        (Depth - 1) * width^2 + 2 * universe_size * width - target_params = 0\n",
        "\n",
        "    Args:\n",
        "        network_depth (int): The number of layers in the network (L). Must be >= 1.\n",
        "        target_param_count (int): The maximum allowable number of parameters (Budget).\n",
        "        universe_size (int): The total number of unique items in the discrete choice set (J).\n",
        "\n",
        "    Returns:\n",
        "        int: The calculated width (number of neurons) for the hidden layers. Returns 0 if no valid solution exists.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If network_depth is less than 1.\n",
        "    \"\"\"\n",
        "    if network_depth < 1:\n",
        "        raise ValueError(f\"Network depth must be >= 1, got {network_depth}\")\n",
        "\n",
        "    # Coefficients for the quadratic equation: ax^2 + bx + c = 0\n",
        "    quad_coeff_a = network_depth - 1\n",
        "    lin_coeff_b = 2 * universe_size\n",
        "    const_coeff_c = -target_param_count\n",
        "\n",
        "    # Handle linear model case (Depth=1)\n",
        "    if quad_coeff_a == 0:\n",
        "        return int(target_param_count / lin_coeff_b) if lin_coeff_b > 0 else 0\n",
        "\n",
        "    discriminant = lin_coeff_b**2 - 4 * quad_coeff_a * const_coeff_c\n",
        "\n",
        "    if discriminant < 0:\n",
        "        return 0\n",
        "\n",
        "    # Quadratic formula\n",
        "    width = (-lin_coeff_b + math.sqrt(discriminant)) / (2 * quad_coeff_a)\n",
        "\n",
        "    return int(width)"
      ],
      "metadata": {
        "id": "QYPs4A7MKJp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [Cell 3] Data Pipeline: Generation and Loading\n",
        "\n",
        "def generate_synthetic_choice_data(\n",
        "    universe_size: int,\n",
        "    choice_set_size: int,\n",
        "    obs_per_set: int,\n",
        "    filepath: str\n",
        ") -> Tuple[np.ndarray, np.ndarray, Dict[ChoiceSet, np.ndarray], List[ChoiceSet]]:\n",
        "    \"\"\"\n",
        "    Generates or loads synthetic discrete choice data based on a Dirichlet-Multinomial process.\n",
        "\n",
        "    This function ensures that every possible combination of the choice set is generated exactly once\n",
        "    to form a complete 'ground truth' dataset.\n",
        "\n",
        "    Args:\n",
        "        universe_size (int): Total number of items available (J).\n",
        "        choice_set_size (int): Number of items in each choice set (K).\n",
        "        obs_per_set (int): Number of simulated user choices per choice set.\n",
        "        filepath (str): Path to the .npz file for caching results.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing:\n",
        "            - availability_vectors (np.ndarray): Input features (One-hot encoding of available items).\n",
        "            - selection_vectors (np.ndarray): Target labels (One-hot encoding of chosen items).\n",
        "            - empirical_freqs (Dict): Mapping of ChoiceSet tuple to empirical probability distribution.\n",
        "            - all_choice_sets (List): List of all unique choice set tuples.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Check Cache\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"Loading cached data from {filepath}...\")\n",
        "        data = np.load(filepath, allow_pickle=True)\n",
        "\n",
        "        # Reconstruct complex objects from numpy arrays\n",
        "        empirical_freqs = data['empirical_freqs'].item()\n",
        "        # Ensure keys are tuples (immutable) for dictionary lookup\n",
        "        choice_sets_list = [tuple(cs) for cs in data['choice_sets']]\n",
        "\n",
        "        return data['X'], data['y'], empirical_freqs, choice_sets_list\n",
        "\n",
        "    # 2. Setup Generators\n",
        "    print(\"Cache not found. initializing data generation...\")\n",
        "    try:\n",
        "        total_combinations = math.comb(universe_size, choice_set_size)\n",
        "    except AttributeError:\n",
        "        # For Python < 3.8 compatibility if needed\n",
        "        import scipy.special\n",
        "        total_combinations = int(scipy.special.comb(universe_size, choice_set_size))\n",
        "\n",
        "    print(f\"Generating {total_combinations} unique choice sets. This may take time.\")\n",
        "\n",
        "    item_indices = range(universe_size)\n",
        "    # Create a generator for all possible combinations C(J, K)\n",
        "    combination_generator = itertools.combinations(item_indices, choice_set_size)\n",
        "\n",
        "    availability_vectors = []\n",
        "    selection_vectors = []\n",
        "    empirical_freqs = {}\n",
        "\n",
        "    # Dirichlet distribution for generating ground truth probabilities\n",
        "    # Concentration = 1 implies a uniform prior over the simplex\n",
        "    dirichlet_dist = tfp.distributions.Dirichlet(concentration=tf.ones(choice_set_size))\n",
        "\n",
        "    # 3. Main Generation Loop\n",
        "    for current_set in tqdm(combination_generator, total=total_combinations, desc=\"Processing Combinations\"):\n",
        "        current_set_tuple = tuple(current_set)\n",
        "\n",
        "        # Create input feature: Vector of size J where 1 indicates item is available\n",
        "        avail_vec = np.zeros(universe_size, dtype='float32')\n",
        "        avail_vec[list(current_set)] = 1.0\n",
        "\n",
        "        # Sample Ground Truth probabilities for this specific set\n",
        "        true_probs = dirichlet_dist.sample().numpy()\n",
        "\n",
        "        # Simulate agent choices based on Ground Truth\n",
        "        simulated_choices_indices = np.random.choice(\n",
        "            choice_set_size,\n",
        "            size=obs_per_set,\n",
        "            p=true_probs\n",
        "        )\n",
        "\n",
        "        # Record empirical frequencies (Ground Truth for Validation RMSE)\n",
        "        counts = np.bincount(simulated_choices_indices, minlength=choice_set_size)\n",
        "        empirical_freqs[current_set_tuple] = counts / obs_per_set\n",
        "\n",
        "        # Generate Training Pairs (X, y)\n",
        "        for relative_index in simulated_choices_indices:\n",
        "            # Map relative index (0..K-1) back to absolute item index (0..J-1)\n",
        "            absolute_item_index = current_set_tuple[relative_index]\n",
        "\n",
        "            label_vec = np.zeros(universe_size, dtype='float32')\n",
        "            label_vec[absolute_item_index] = 1.0\n",
        "\n",
        "            availability_vectors.append(avail_vec)\n",
        "            selection_vectors.append(label_vec)\n",
        "\n",
        "    # 4. Finalize and Save\n",
        "    X_final = np.array(availability_vectors)\n",
        "    y_final = np.array(selection_vectors)\n",
        "    choice_sets_final = list(empirical_freqs.keys())\n",
        "\n",
        "    print(f\"Saving generated data to {filepath}...\")\n",
        "    np.savez_compressed(\n",
        "        filepath,\n",
        "        X=X_final,\n",
        "        y=y_final,\n",
        "        empirical_freqs=empirical_freqs,\n",
        "        choice_sets=choice_sets_final\n",
        "    )\n",
        "\n",
        "    return X_final, y_final, empirical_freqs, choice_sets_final"
      ],
      "metadata": {
        "id": "KzvJmTSaKNQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [Cell 4] Model Definition: Featureless DeepHalo\n",
        "\n",
        "def build_featureless_deep_halo_model(\n",
        "    universe_size: int,\n",
        "    hidden_width: int,\n",
        "    network_depth: int\n",
        ") -> models.Model:\n",
        "    \"\"\"\n",
        "    Constructs the DeepHalo neural network architecture designed for featureless discrete choice.\n",
        "\n",
        "    Key Architectural Features:\n",
        "    1.  **Quadratic Activation**: Uses x^2 instead of ReLU to explicitly model interaction effects.\n",
        "    2.  **ResNet Topology**: Uses skip connections for gradient flow.\n",
        "    3.  **Availability Masking**: Ensures probability of unavailable items is strictly zero.\n",
        "\n",
        "    Args:\n",
        "        universe_size (int): Size of the item universe (Input/Output dimension).\n",
        "        hidden_width (int): Number of neurons in hidden layers.\n",
        "        network_depth (int): Total number of non-linear transformations.\n",
        "\n",
        "    Returns:\n",
        "        models.Model: A compiled Keras functional model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Input: Binary vector indicating item availability\n",
        "    inputs = layers.Input(shape=(universe_size,), name='Availability_Input')\n",
        "\n",
        "    # Initial Projection: Linear mapping to hidden dimension\n",
        "    # Note: use_bias=False is used to adhere strictly to parameter budget constraints logic\n",
        "    x = layers.Dense(hidden_width, use_bias=False, name='Projection_Layer')(inputs)\n",
        "\n",
        "    # Stacked Residual Blocks\n",
        "    for i in range(1, network_depth + 1):\n",
        "        # Identity path\n",
        "        shortcut = x\n",
        "\n",
        "        # Quadratic Activation: Explicitly models pairwise interactions\n",
        "        x = layers.Lambda(lambda t: tf.square(t), name=f'Quadratic_Activation_{i}')(x)\n",
        "\n",
        "        # Linear Transformation (Mixing)\n",
        "        x = layers.Dense(hidden_width, use_bias=False, name=f'Mixing_Layer_{i}')(x)\n",
        "\n",
        "        # Residual Connection\n",
        "        x = layers.Add(name=f'Residual_Add_{i}')([shortcut, x])\n",
        "\n",
        "    # Output Projection: Map back to item universe size\n",
        "    logits = layers.Dense(universe_size, use_bias=False, name='Logit_Projection')(x)\n",
        "\n",
        "    # --- Masking Mechanism ---\n",
        "    # We must ensure the model cannot assign probability to items not in the input set.\n",
        "    # 1. Cast input (0/1 floats) to boolean mask\n",
        "    mask_boolean = layers.Lambda(\n",
        "        lambda t: tf.cast(t, dtype=tf.bool),\n",
        "        name='Create_Boolean_Mask'\n",
        "    )(inputs)\n",
        "\n",
        "    # 2. Apply Mask: Replace logits of unavailable items with -1e9 (approx negative infinity)\n",
        "    # This ensures exp(logit) is effectively 0 during Softmax\n",
        "    masked_logits = layers.Lambda(\n",
        "        lambda args: tf.where(args[0], args[1], -1e9),\n",
        "        output_shape=(universe_size,),\n",
        "        name='Apply_Availability_Mask'\n",
        "    )([mask_boolean, logits])\n",
        "\n",
        "    # Probability Output\n",
        "    outputs = layers.Softmax(name='Probability_Distribution')(masked_logits)\n",
        "\n",
        "    return models.Model(inputs=inputs, outputs=outputs, name=\"DeepHalo_Model\")"
      ],
      "metadata": {
        "id": "rho4XIWoKVzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [Cell 5] Evaluation Callback: Distribution Matching\n",
        "\n",
        "class DistributionMatchingRMSECallback(Callback):\n",
        "    \"\"\"\n",
        "    A custom Keras Callback to evaluate the Root Mean Squared Error (RMSE)\n",
        "    between predicted probabilities and ground-truth empirical frequencies.\n",
        "\n",
        "    Unlike standard validation which checks single-sample accuracy, this callback:\n",
        "    1. Iterates through ALL unique choice sets.\n",
        "    2. Predicts the full probability distribution for each set.\n",
        "    3. Compares it against the true Dirichlet distribution from data generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        unique_choice_sets: List[ChoiceSet],\n",
        "        empirical_distributions: Dict[ChoiceSet, np.ndarray],\n",
        "        check_interval: int = 10\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the callback.\n",
        "\n",
        "        Args:\n",
        "            unique_choice_sets (List[ChoiceSet]): List of all unique choice set tuples.\n",
        "            empirical_distributions (Dict): Lookup table for ground truth probabilities.\n",
        "            check_interval (int): Epoch frequency to run this expensive calculation.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.unique_choice_sets = unique_choice_sets\n",
        "        self.empirical_distributions = empirical_distributions\n",
        "        self.check_interval = check_interval\n",
        "        self.rmse_history: List[float] = []\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs: Optional[Dict] = None):\n",
        "        # Skip evaluation if not the correct interval\n",
        "        if (epoch + 1) % self.check_interval != 0:\n",
        "            return\n",
        "\n",
        "        # Accumulator for squared errors\n",
        "        batch_squared_errors = []\n",
        "\n",
        "        # NOTE: We iterate one by one for clarity, but batched inference\n",
        "        # would be used in a production environment for speed.\n",
        "        for choice_set in self.unique_choice_sets:\n",
        "            # Construct input vector for this specific choice set\n",
        "            input_vec = np.zeros((1, self.model.input_shape[1]), dtype='float32')\n",
        "            input_vec[0, list(choice_set)] = 1.0\n",
        "\n",
        "            # Get model prediction\n",
        "            pred_probs = self.model.predict(input_vec, verbose=0)[0]\n",
        "\n",
        "            # Filter predictions to only relevant items\n",
        "            relevant_preds = pred_probs[list(choice_set)]\n",
        "\n",
        "            # Get Ground Truth\n",
        "            true_freqs = self.empirical_distributions[choice_set]\n",
        "\n",
        "            # Compute Squared Error for this set\n",
        "            error = np.square(relevant_preds - true_freqs)\n",
        "            batch_squared_errors.append(np.mean(error))\n",
        "\n",
        "        # Compute global RMSE\n",
        "        current_rmse = np.sqrt(np.mean(batch_squared_errors))\n",
        "        self.rmse_history.append(current_rmse)\n",
        "\n",
        "        print(f\" â€” [Epoch {epoch+1}] Validation Distribution RMSE: {current_rmse:.6f}\")"
      ],
      "metadata": {
        "id": "4PVLQRYgKY5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [Cell 6] Experiment Orchestration\n",
        "\n",
        "def run_comparative_experiment(\n",
        "    x_data: np.ndarray,\n",
        "    y_data: np.ndarray,\n",
        "    empirical_lookup: Dict,\n",
        "    choice_sets_list: List,\n",
        "    config: ModelConfig\n",
        ") -> Dict[int, Dict[int, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the training of multiple models across different parameter budgets and depths.\n",
        "\n",
        "    Args:\n",
        "        x_data (np.ndarray): Training features.\n",
        "        y_data (np.ndarray): Training labels.\n",
        "        empirical_lookup (Dict): Ground truth data for validation.\n",
        "        choice_sets_list (List): List of choice sets for validation.\n",
        "        config (ModelConfig): Dictionary containing hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Nested dictionary containing experiment results keyed by [Budget][Depth].\n",
        "    \"\"\"\n",
        "    experiment_results = {}\n",
        "\n",
        "    # Determine validation frequency\n",
        "    validation_interval = 1 if DEBUG_MODE else 10\n",
        "\n",
        "    for budget in config[\"param_budgets\"]:\n",
        "        experiment_results[budget] = {}\n",
        "\n",
        "        for depth in config[\"network_depths\"]:\n",
        "            print(f\"\\n{'='*40}\")\n",
        "            print(f\"Experiment: Budget={budget}, Depth={depth}\")\n",
        "            print(f\"{'='*40}\")\n",
        "\n",
        "            # 1. Calculate Topology Constraints\n",
        "            layer_width = calculate_layer_width(\n",
        "                network_depth=depth,\n",
        "                target_param_count=budget,\n",
        "                universe_size=config[\"universe_size\"]\n",
        "            )\n",
        "\n",
        "            if layer_width <= 0:\n",
        "                print(f\"Skipping configuration: Cannot satisfy budget {budget} with depth {depth}.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"-> Architecture: {layer_width} neurons per layer.\")\n",
        "\n",
        "            # 2. Initialize Model\n",
        "            model = build_featureless_deep_halo_model(\n",
        "                universe_size=config[\"universe_size\"],\n",
        "                hidden_width=layer_width,\n",
        "                network_depth=depth\n",
        "            )\n",
        "\n",
        "            # 3. Compile\n",
        "            # We use MSE because we are regressing towards a probability distribution\n",
        "            model.compile(\n",
        "                optimizer=optimizers.Adam(learning_rate=config[\"learning_rate\"]),\n",
        "                loss='mean_squared_error'\n",
        "            )\n",
        "\n",
        "            # 4. Setup Validator\n",
        "            rmse_validator = DistributionMatchingRMSECallback(\n",
        "                unique_choice_sets=choice_sets_list,\n",
        "                empirical_distributions=empirical_lookup,\n",
        "                check_interval=validation_interval\n",
        "            )\n",
        "\n",
        "            # 5. Train\n",
        "            model.fit(\n",
        "                x_data, y_data,\n",
        "                batch_size=config[\"batch_size\"],\n",
        "                epochs=config[\"epochs\"],\n",
        "                callbacks=[rmse_validator],\n",
        "                verbose=0  # Suppress default Keras bar to keep output clean\n",
        "            )\n",
        "\n",
        "            # 6. Log Results\n",
        "            final_score = rmse_validator.rmse_history[-1] if rmse_validator.rmse_history else 0.0\n",
        "            experiment_results[budget][depth] = {\n",
        "                'rmse_history': rmse_validator.rmse_history,\n",
        "                'final_rmse': final_score,\n",
        "                'layer_width': layer_width\n",
        "            }\n",
        "            print(f\"-> Result: Final RMSE = {final_score:.5f}\")\n",
        "\n",
        "    return experiment_results"
      ],
      "metadata": {
        "id": "Iib8581yKccA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [Cell 7] Visualization of Results\n",
        "\n",
        "def plot_performance_metrics_repro(results_data: Dict, depths: List[int], is_debug: bool):\n",
        "    \"\"\"\n",
        "    Plots the comparative results\n",
        "    \"\"\"\n",
        "    # --- Setup Style ---\n",
        "    sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
        "    check_interval = 1 if is_debug else 100\n",
        "    budgets = sorted(results_data.keys())\n",
        "    n_depths = len(depths)\n",
        "\n",
        "    # Define Gradient Palettes\n",
        "    blues_palette = sns.color_palette(\"Blues\", n_colors=n_depths + 2)[2:]\n",
        "    oranges_palette = sns.color_palette(\"YlOrBr\", n_colors=n_depths + 2)[2:]\n",
        "\n",
        "    # Map Budget -> Palette\n",
        "    palette_map = {\n",
        "        budgets[0]: blues_palette,      # e.g. 200k or 500 (debug)\n",
        "        budgets[1]: oranges_palette     # e.g. 500k or 1000 (debug)\n",
        "    } if len(budgets) >= 2 else {budgets[0]: blues_palette}\n",
        "\n",
        "    # ==========================================\n",
        "    # Figure: Effect of Model Depth\n",
        "    # ==========================================\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Line Styles\n",
        "    styles = {\n",
        "        budgets[0]: {'color': '#4c72b0', 'marker': 'o', 'label': f'{int(budgets[0]/1000)}k'},\n",
        "        budgets[1]: {'color': '#dd8452', 'marker': '*', 'label': f'{int(budgets[1]/1000)}k'}\n",
        "    } if len(budgets) >= 2 else {budgets[0]: {'color': '#4c72b0', 'marker': 'o', 'label': str(budgets[0])}}\n",
        "\n",
        "    for budget in budgets:\n",
        "        # Extract y-values (RMSE) for x-axis (Depths)\n",
        "        rmse_values = []\n",
        "        valid_depths = []\n",
        "        for d in depths:\n",
        "            if d in results_data[budget]:\n",
        "                rmse_values.append(results_data[budget][d]['final_rmse'])\n",
        "                valid_depths.append(d)\n",
        "\n",
        "        # Plot Line\n",
        "        plt.plot(\n",
        "            valid_depths,\n",
        "            rmse_values,\n",
        "            marker=styles[budget]['marker'],\n",
        "            color=styles[budget]['color'],\n",
        "            label=styles[budget]['label'],\n",
        "            linewidth=1.5,\n",
        "            markersize=6\n",
        "        )\n",
        "\n",
        "    plt.xlabel('Depth')\n",
        "    plt.ylabel('Training RMSE')\n",
        "    plt.xticks(depths)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ==========================================\n",
        "    # Figure: Training Loss Curve Across Epochs\n",
        "    # ==========================================\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    for b_idx, budget in enumerate(budgets):\n",
        "        current_palette = palette_map[budget]\n",
        "\n",
        "        # Sort depths to ensure color gradient matches depth order\n",
        "        sorted_experiment_depths = sorted(results_data[budget].keys())\n",
        "\n",
        "        for i, depth in enumerate(sorted_experiment_depths):\n",
        "            history = results_data[budget][depth]['rmse_history']\n",
        "            epochs_x = np.arange(1, len(history) + 1) * check_interval\n",
        "\n",
        "            # Assign color from palette based on depth index\n",
        "            # If debug mode has fewer depths, we pick from the beginning of palette\n",
        "            color = current_palette[i] if i < len(current_palette) else current_palette[-1]\n",
        "\n",
        "            label_str = f\"{int(budget/1000)}k Dep {depth}\"\n",
        "\n",
        "            plt.plot(\n",
        "                epochs_x,\n",
        "                history,\n",
        "                color=color,\n",
        "                label=label_str,\n",
        "                linewidth=1.5,\n",
        "                alpha=0.9\n",
        "            )\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Training RMSE')\n",
        "\n",
        "    # Create a legend that fits well (top right)\n",
        "    plt.legend(loc='upper right', frameon=True, framealpha=0.9)\n",
        "    plt.grid(True, which='major', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YbPnmofdMm6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [Cell 8] Formal Unit Testing\n",
        "\n",
        "# Note: This test suite relies on the imports and configurations defined in Cell 1.\n",
        "\n",
        "class TestDeepHaloReproduction(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    A comprehensive test suite to validate the core functional components of the DeepHalo project,\n",
        "    ensuring robustness of the math, data generation, and model architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        Fixture Setup: Runs before EACH test method.\n",
        "        Sets up controlled, small-scale test parameters and defines a temporary filename.\n",
        "        \"\"\"\n",
        "        # Use small parameters for quick verification\n",
        "        self.test_universe_size = 10\n",
        "        self.test_choice_set_size = 4\n",
        "        self.test_obs_per_set = 50\n",
        "        # Define a temporary test file path\n",
        "        self.test_filename = \"test_suite_temp_data.npz\"\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"\n",
        "        Fixture Teardown: Runs after EACH test method.\n",
        "        Cleans up temporary artifacts, such as the generated .npz file, to ensure test isolation.\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.test_filename):\n",
        "            os.remove(self.test_filename)\n",
        "\n",
        "    def test_calculate_layer_width_logic(self):\n",
        "        \"\"\"\n",
        "        Test: Verifies the correctness of the quadratic equation solver (calculate_layer_width).\n",
        "        Checks standard cases, linear models (Depth=1), and invalid budget handling.\n",
        "        \"\"\"\n",
        "        # Case 1: Standard case (Depth=3, Budget=2000, J=20)\n",
        "        # Expected width should be positive\n",
        "        width = calculate_layer_width(network_depth=3, target_param_count=2000, universe_size=20)\n",
        "        self.assertGreater(width, 0, \"Calculated width must be a positive integer.\")\n",
        "\n",
        "        # Case 2: Linear model equivalent (Depth=1)\n",
        "        # Formula simplifies to 2 * J * W = Budget. W = 200 / (2 * 10) = 10\n",
        "        width_linear = calculate_layer_width(network_depth=1, target_param_count=200, universe_size=10)\n",
        "        self.assertEqual(width_linear, 10, \"Width calculation for Depth=1 (Linear) is incorrect.\")\n",
        "\n",
        "        # Case 3: Impossible budget (should return 0)\n",
        "        width_impossible = calculate_layer_width(network_depth=5, target_param_count=10, universe_size=100)\n",
        "        self.assertEqual(width_impossible, 0, \"Should return 0 when the budget is too small for the architecture.\")\n",
        "\n",
        "        # Case 4: Invalid input (Depth=0 must raise ValueError)\n",
        "        with self.assertRaises(ValueError):\n",
        "            calculate_layer_width(network_depth=0, target_param_count=1000, universe_size=20)\n",
        "\n",
        "    def test_data_generation_integrity(self):\n",
        "        \"\"\"\n",
        "        Test: Verifies data generation pipeline for correct shapes, combinatorial logic, and caching.\n",
        "        \"\"\"\n",
        "        # Run 1: Generate data and cache\n",
        "        X, y, freqs, choices = generate_synthetic_choice_data(\n",
        "            self.test_universe_size, self.test_choice_set_size, self.test_obs_per_set, self.test_filename\n",
        "        )\n",
        "\n",
        "        # Check 1: File creation\n",
        "        self.assertTrue(os.path.exists(self.test_filename), \"Data file was not created/cached.\")\n",
        "\n",
        "        # Check 2: Shape consistency (X and y must have the same number of samples)\n",
        "        self.assertEqual(X.shape[0], y.shape[0], \"The number of samples in X and y do not match.\")\n",
        "\n",
        "        # Check 3: Combinatorial Logic (e.g., C(10, 4) = 210)\n",
        "        expected_combinations = math.comb(self.test_universe_size, self.test_choice_set_size)\n",
        "        self.assertEqual(len(choices), expected_combinations, \"The number of unique combinations generated is incorrect.\")\n",
        "\n",
        "        # Check 4: Caching behavior (Teardown ensures the file is removed afterward)\n",
        "\n",
        "    def test_model_architecture_spec(self):\n",
        "        \"\"\"\n",
        "        Test: Verifies the Keras model construction, checking input/output shapes and critical layer presence.\n",
        "        \"\"\"\n",
        "        test_J = 5\n",
        "        test_W = 8\n",
        "        model = build_featureless_deep_halo_model(universe_size=test_J, hidden_width=test_W, network_depth=3)\n",
        "\n",
        "        # Check 1: Input shape (should be (None, J))\n",
        "        self.assertEqual(model.input_shape[1], test_J, \"Model input layer dimension (J) is incorrect.\")\n",
        "\n",
        "        # Check 2: Output shape (should be (None, J))\n",
        "        self.assertEqual(model.output_shape[1], test_J, \"Model output layer dimension (J) is incorrect.\")\n",
        "\n",
        "        # Check 3: Critical layer presence (Availability Masking is mandatory)\n",
        "        layer_names = [layer.name for layer in model.layers]\n",
        "        self.assertIn(\"Apply_Availability_Mask\", layer_names, \"DeepHalo architecture lacks the mandatory availability masking layer.\")\n",
        "\n",
        "        # Check 4: Critical layer presence (Quadratic Activation)\n",
        "        self.assertTrue(any(\"Quadratic_Activation\" in name for name in layer_names), \"DeepHalo architecture lacks the quadratic activation layer.\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# RUN TEST SUITE\n",
        "# ==========================================\n",
        "if __name__ == '__main__':\n",
        "    print(\"Running Formal Unit Tests...\")\n",
        "    # This command runs unittest.main() without exiting the Jupyter kernel\n",
        "    unittest.main(argv=['first-arg-is-ignored'], verbosity=2, exit=False)"
      ],
      "metadata": {
        "id": "G0C_7cpsPlG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [Cell 9] FINAL EXECUTION PIPELINE\n",
        "\n",
        "total_start_time = time.time()\n",
        "print(f\"--- STARTING FULL PIPELINE (DEBUG_MODE={DEBUG_MODE}) ---\")\n",
        "\n",
        "# 1. Load/Generate Data (Uses CONFIG settings from Cell 1)\n",
        "print(\"\\n>>> STEP 1: Preparing Data...\")\n",
        "X_train, y_train, empirical_freqs, all_choice_sets = generate_synthetic_choice_data(\n",
        "    CONFIG[\"universe_size\"],\n",
        "    CONFIG[\"choice_set_size\"],\n",
        "    CONFIG[\"obs_per_set\"],\n",
        "    DATA_FILENAME\n",
        ")\n",
        "\n",
        "# 2. Run Experiments (Uses functions defined in Cells 5 & 6)\n",
        "print(\"\\n>>> STEP 2: Running Comparative Experiments...\")\n",
        "experiment_results = run_comparative_experiment(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    empirical_freqs,\n",
        "    all_choice_sets,\n",
        "    CONFIG\n",
        ")\n",
        "\n",
        "# 3. Plot Results (Uses function defined in Cell 7)\n",
        "print(\"\\n>>> STEP 3: Visualizing Results...\")\n",
        "# Pass the global DEBUG_MODE to the plotter for correct check interval calculation\n",
        "plot_performance_metrics_repro(experiment_results, CONFIG[\"network_depths\"], DEBUG_MODE)\n",
        "\n",
        "print(f\"\\n--- PIPELINE COMPLETED in {time.time() - total_start_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "id": "nt4EI3OPPs51"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}